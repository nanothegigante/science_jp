# 科学技術白書（PDF版）コーパス構築および分布意味分析パイプライン

本リポジトリでは、2017〜2025年の『科学技術白書（科学技術・イノベーション白書）』PDF版を対象に、テキストコーパスを構築し、「科学」という語の分布意味的特徴を分析するための処理手順を整理する。

本ドキュメントでは、以下の工程について記録する。

1. データ収集（PDF版のみ）
2. PDFからテキスト抽出
3. テキスト前処理
4. 形態素解析（SudachiPy）
5. Word2Vecによる分布意味分析
6. 近傍語抽出

HTML版の収集・処理については本プロジェクトでは扱わない。（WARPに格納された過去のHTML版科学技術白書についても、同様の分析が行えるように改良を進める予定。）

---

## 1. データ収集

### 対象
- 科学技術白書 | 科学技術・イノベーション白書（2017年〜2025年）
  - 文部科学省公式サイトより `https://www.mext.go.jp/b_menu/hakusho/html/kagaku.htm`
- PDF版のみ使用

### 保存ディレクトリ構造

```text
corpus/
  pdf/
    2017_h29/
      *.pdf
    2018_h30/
      *.pdf
    2019_r1/
      *.pdf
    ...
```

各年ごとにフォルダを分け、その中に複数PDF（本文）を格納した。特集記事や寄稿記事なども収集の対象としたが、ひとつ2025年の付録記事である『白書のテキストマイニングによる政策動向分析』についてのみ、白書の内容をメタに見るという、本分析の志すところと同じ視点であるという理由から、収集対象から除外した。

---

## 2. PDFからテキスト抽出

### 使用ライブラリと実行スクリプト
- ライブラリ: `PyMuPDF（fitz）`
- スクリプト: `pdftotxt.py`

### 目的
- PDFレイアウトを考慮しつつテキストを抽出
- 1カラム／2カラム文書の双方に対応
- 年ごとに統合テキストを生成

### 出力構造

```text
txt_raw/
  2017.txt
  2018.txt
  ...
```

### 特徴
- ページ単位でテキストブロックを取得
- 座標情報に基づき読み順を調整
- 1カラム／2カラムを自動判定
- 年ごとにPDFを統合

---

## 3. テキスト前処理

### 目的
- 図表由来ノイズの削減
- 不要行の削除
- 改行の整理（完全除去は行わない）

### 実施内容
`txt_clean`には、`txt_raw`からさらに以下の処理を行ったテキストファイルを格納
- 数字・記号比率が極端に高い行の除去
- 「図」「表」「出典」などのキャプション除去
- 連続空白の圧縮

### 出力
``` text
txt_clean/
  2017.clean.txt
  2018.clean.txt
  ...
```

---

## 4. 形態素解析

### 使用ライブラリと実行スクリプト
- ライブラリ: `SudachiPy sudachidict_core`
- スクリプト: `tokenise.py`

### 目的
- 白書テキストをトークン列へ変換
- Word2Vec入力形式へ整列

### 課題と対応
`SudachiPy`には入力バイト数制限（約49KB）があるため、
- 段落単位でテキストを分割
- 分割後に逐次トークン化
- ストリーミング出力
を行った。

### 出力
```text
tokens/
  2017.tokens.txt
  2018.tokens.txt
  ...
```

### トークン数（参考）
```text
2017: 200,534
2018: 174,026
2019: 136,827
2020: 149,978
2021: 135,119
2022: 138,771
2023: 121,427
2024: 153,096
2025: 151,151
```
全体で約136万トークン。

---

## 5. Word2Vecによる分布意味モデル構築

### 使用ライブラリと実行スクリプト
- ライブラリ: `gensim`
- スクリプト: `train_word2vec_yearly.py`

### 処理内容
- 各年のトークン列を読み込む
- Skip-gramモデルで学習

### パラメータ
```text
- vector_size = 200
- window = 5
- min_count = 5
- sg = 1（skip-gram）
- epochs = 20
```

### 学習単位
- 年ごとに個別モデルを作成

### 出力
```text
models/
  2017.model
  2018.model
  ...
```

---

## 6. 「科学」の近傍語抽出

各年モデルから、
```python
model.wv.most_similar("科学", topn=15)
```
を実行し、年別の近傍語ランキングを取得。

### 実行スクリプト
- `print_neighbors.py`

### 目的
- 「科学」がどの語群と意味的に近接しているかを確認（`most_similar("科学")` を実行）
- 政策文書内における意味構造を観察
- 時系列的傾向の把握（but 限定的）

### 出力例
「科学」とのコサイン類似度（-1 ~ 1）が高い上位15の語彙とそのコサイン類似度

```text

====================
2017
====================
数理              0.960
戦               0.956
橋渡し             0.955
アカデミア           0.953
●               0.952
⑥               0.952
航空機             0.951
素材              0.951
水産業             0.951
食品              0.951
生物学             0.950
計算              0.950
イノベーショ          0.949
計測              0.948
イメージ            0.948

====================
2018
====================
物理学             0.898
生命科学            0.891
計算機             0.891
材料              0.883
数学              0.882
基礎              0.879
医学              0.866
研究              0.862
機関              0.851
研究所             0.843
推進              0.841
臨床              0.840
地域              0.835
づくり             0.831
製品              0.828

====================
2019
====================
総               0.975
調査              0.969
サイエンス           0.967
８               0.957
出場              0.954
火山              0.950
会               0.948
選手              0.946
島               0.946
物理              0.945
各国              0.943
化学              0.943
数学              0.940
／               0.936
2018            0.936
...
...
```


---

## 7. 可視化（未着手）

`plot_semantic_space.py`

---

## 現時点での課題（20260220）
- 単漢字、意味を持たない文字や記号由来のノイズ語が混入
- 複合語（例：科学技術、研究開発）が分割されている場合がある
- 品詞フィルタリング未適用
- コーパスが各年ゆえにコサイン類似度が高く出る傾向

今後の改善策として：
- 前処理の精緻化
  - 品詞制限（名詞・動詞・形容詞）
  - 記号除去
  - 複合語保護 
- 1文字語の除外
を検討する。

---

## 分析上の注意
- 年別モデルは整列（alignment）を行っていないため、ベクトル空間間の距離比較は行っていない
- 本分析は「近傍語傾向の観察」に限定される

---

## 今後の拡張可能性
- 戦後以降のデータの追加
- 共起語分析との比較
- ベクトル空間整列による意味変位量算出
- UMAP / PCAによる可視化
- 概念クラスタリング






